{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from GDTR import GeneticDecisionTreeRegressor\n",
    "from or_gym.envs.supply_chain.inventory_management import InvManagementMasterEnv\n",
    "import random\n",
    "from scipy.optimize import minimize\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_inventory_state(env):\n",
    "        '''\n",
    "        Get current state of the system: Inventory position at each echelon\n",
    "        Inventory at hand + Pipeline inventory - backlog up to the current stage \n",
    "        (excludes last stage since no inventory there, nor replenishment orders placed there).\n",
    "        '''\n",
    "        n = env.period\n",
    "        m = env.num_stages\n",
    "        #print(env.I[n, :])\n",
    "        if n>=1:\n",
    "            IP = np.cumsum(env.I[n,:] + env.T[n,:] - env.B[n-1,:-1])\n",
    "            #print(IP)\n",
    "        else:\n",
    "            IP = np.cumsum(env.I[n,:] + env.T[n,:])\n",
    "            #print(IP)\n",
    "        env.state = IP\n",
    "        return env.state\n",
    "        \n",
    "        \n",
    "def base_stock_action(env ,z):\n",
    "        '''\n",
    "        Sample action (number of units to request) based on a base-stock policy (order up to z policy)\n",
    "        z = [integer list; dimension |Stages| - 1] base stock level (no inventory at the last stage)\n",
    "        '''\n",
    "        n = env.period\n",
    "        c = env.supply_capacity\n",
    "        m = env.num_stages\n",
    "        IP = _update_inventory_state(env) # extract inventory position (current state)\n",
    "        \n",
    "        try:\n",
    "            dimz = len(z)\n",
    "        except:\n",
    "            dimz = 1\n",
    "        assert dimz == m-1, \"Wrong dimension on base stock level vector. Should be # Stages - 1.\"\n",
    "        \n",
    "        #print(type(IP))\n",
    "        #print(IP)\n",
    "        # calculate total inventory position at the beginning of period n\n",
    "        R = z - IP # replenishmet order to reach zopt\n",
    "\n",
    "        # check if R can actually be fulfilled (capacity and inventory constraints)\n",
    "        Im1 = np.append(env.I[n,1:], np.Inf) # available inventory at the m+1 stage\n",
    "                                            # NOTE: last stage has unlimited raw materials\n",
    "        Rpos = np.column_stack((np.zeros(len(R)),R)) # augmented materix to get replenishment only if positive\n",
    "        A = np.column_stack((c, np.max(Rpos,axis=1), Im1)) # augmented matrix with c, R, and I_m+1 as columns\n",
    "        \n",
    "        R = np.min(A, axis = 1) # replenishmet order to reach zopt (capacity constrained)\n",
    "        \n",
    "        return R\n",
    "\n",
    "\n",
    "def min_max_action(env, z_min, z_max): # (s, S) policy\n",
    "    '''\n",
    "    Sample action (number of units to request) based on a min-max policy (order up to z_max if inv is below z_min)\n",
    "    z = [integer list; dimension |Stages| - 1] base stock level (no inventory at the last stage)\n",
    "    '''\n",
    "    assert len(z_min) == len(z_max)\n",
    "    \n",
    "    n = env.period\n",
    "    c = env.supply_capacity\n",
    "    m = env.num_stages\n",
    "    IP = _update_inventory_state(env) # extract inventory position (current state)\n",
    "    \n",
    "    try:\n",
    "        dimz = len(z_min)\n",
    "    except:\n",
    "        dimz = 1\n",
    "    assert dimz == m-1, \"Wrong dimension on base stock level vector. Should be # Stages - 1.\"\n",
    "    \n",
    "    R = np.zeros(3)\n",
    "    #print(type(IP))\n",
    "    #print(IP)\n",
    "    # calculate total inventory position at the beginning of period n\n",
    "    for i in range(len(IP)):\n",
    "        if IP[i] < z_min[i]:\n",
    "            R[i] = z_max[i] - IP[i] # replenishmet order to reach z_max\n",
    "        else:\n",
    "            R[i] = 0\n",
    "\n",
    "    # check if R can actually be fulfilled (capacity and inventory constraints)\n",
    "    Im1 = np.append(env.I[n,1:], np.Inf) # available inventory at the m+1 stage\n",
    "                                        # NOTE: last stage has unlimited raw materials\n",
    "    Rpos = np.column_stack((np.zeros(len(R)),R)) # augmented materix to get replenishment only if positive\n",
    "    A = np.column_stack((c, np.max(Rpos,axis=1), Im1)) # augmented matrix with c, R, and I_m+1 as columns\n",
    "    \n",
    "    R = np.min(A, axis = 1) # replenishmet order to reach zopt (capacity constrained)\n",
    "    \n",
    "    return R\n",
    "\n",
    "\n",
    "def R_S_action(env, R_period, S): # kinda like base-stock, but without the continuous review\n",
    "    '''\n",
    "    Inventory level is reviewed every R_period periods.\n",
    "    Right after the review an order is placed bringing the inventory level to the predefined level S\n",
    "    '''\n",
    "    n = env.period\n",
    "    \n",
    "    if n % R_period != 0:\n",
    "        R = np.zeros(3) # 3 being the \n",
    "        \n",
    "    else:\n",
    "        c = env.supply_capacity\n",
    "        m = env.num_stages\n",
    "        IP = _update_inventory_state(env) # extract inventory position (current state)\n",
    "        \n",
    "        try:\n",
    "            dimz = len(S)\n",
    "        except:\n",
    "            dimz = 1\n",
    "        assert dimz == m-1, \"Wrong dimension on base stock level vector. Should be # Stages - 1.\"\n",
    "        \n",
    "        #print(type(IP))\n",
    "        #print(IP)\n",
    "        # calculate total inventory position at the beginning of period n\n",
    "        R = S - IP # replenishmet order to reach zopt\n",
    "\n",
    "        # check if R can actually be fulfilled (capacity and inventory constraints)\n",
    "        Im1 = np.append(env.I[n,1:], np.Inf) # available inventory at the m+1 stage\n",
    "                                            # NOTE: last stage has unlimited raw materials\n",
    "        Rpos = np.column_stack((np.zeros(len(R)),R)) # augmented materix to get replenishment only if positive\n",
    "        A = np.column_stack((c, np.max(Rpos,axis=1), Im1)) # augmented matrix with c, R, and I_m+1 as columns\n",
    "        \n",
    "        R = np.min(A, axis = 1) # replenishmet order to reach zopt (capacity constrained)\n",
    "    \n",
    "    return R\n",
    "\n",
    "\n",
    "def R_s_S_action(env, R_period, s, S): # like min-max, but without the continuous review\n",
    "    '''\n",
    "    Inventory level is reviewed every R_period periods and as soon as it passes a reorder point s, \n",
    "    an order is placed bringing the inventory level to the predefined level S. \n",
    "    '''\n",
    "    n = env.period\n",
    "    \n",
    "    if n % R_period:\n",
    "        R = np.zeros(3)\n",
    "    \n",
    "    else:\n",
    "        assert len(s) == len(S)\n",
    "    \n",
    "        c = env.supply_capacity\n",
    "        m = env.num_stages\n",
    "        IP = _update_inventory_state(env) # extract inventory position (current state)\n",
    "        \n",
    "        try:\n",
    "            dimz = len(s)\n",
    "        except:\n",
    "            dimz = 1\n",
    "        assert dimz == m-1, \"Wrong dimension on base stock level vector. Should be # Stages - 1.\"\n",
    "        \n",
    "        R = np.zeros(3)\n",
    "        #print(type(IP))\n",
    "        #print(IP)\n",
    "        # calculate total inventory position at the beginning of period n\n",
    "        for i in range(len(IP)):\n",
    "            if IP[i] < s[i]:\n",
    "                R[i] = S[i] - IP[i] # replenishmet order to reach z_max\n",
    "            else:\n",
    "                R[i] = 0\n",
    "\n",
    "        # check if R can actually be fulfilled (capacity and inventory constraints)\n",
    "        Im1 = np.append(env.I[n,1:], np.Inf) # available inventory at the m+1 stage\n",
    "                                            # NOTE: last stage has unlimited raw materials\n",
    "        Rpos = np.column_stack((np.zeros(len(R)),R)) # augmented materix to get replenishment only if positive\n",
    "        A = np.column_stack((c, np.max(Rpos,axis=1), Im1)) # augmented matrix with c, R, and I_m+1 as columns\n",
    "        \n",
    "        R = np.min(A, axis = 1) # replenishmet order to reach zopt (capacity constrained)\n",
    "    \n",
    "    return R\n",
    "\n",
    "\n",
    "def r_Q_action(env, r, Q):\n",
    "    '''\n",
    "    Inventory level is reviewed continuously, \n",
    "    and as  soon as  inventory level reaches the threshold r, an order of size Q is placed.\n",
    "    '''\n",
    "    n = env.period\n",
    "    c = env.supply_capacity\n",
    "    m = env.num_stages\n",
    "    IP = _update_inventory_state(env) # extract inventory position (current state)\n",
    "    \n",
    "    try:\n",
    "        dimz = len(Q)\n",
    "    except:\n",
    "        dimz = 1\n",
    "    assert dimz == m-1, \"Wrong dimension on base stock level vector. Should be # Stages - 1.\"\n",
    "    \n",
    "    R = np.zeros(3)\n",
    "    \n",
    "    for i in range(len(IP)):\n",
    "        if IP[i] < r[i]:\n",
    "            R[i] = Q[i]\n",
    "        else:\n",
    "            R[i] = 0\n",
    "\n",
    "    # check if R can actually be fulfilled (capacity and inventory constraints)\n",
    "    Im1 = np.append(env.I[n,1:], np.Inf) # available inventory at the m+1 stage\n",
    "                                        # NOTE: last stage has unlimited raw materials\n",
    "    Rpos = np.column_stack((np.zeros(len(R)),R)) # augmented materix to get replenishment only if positive\n",
    "    A = np.column_stack((c, np.max(Rpos,axis=1), Im1)) # augmented matrix with c, R, and I_m+1 as columns\n",
    "    \n",
    "    R = np.min(A, axis = 1) # replenishmet order to reach zopt (capacity constrained)\n",
    "    \n",
    "    return R\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimised Base-Stock Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_stock_black_box(base_stock):\n",
    "    env = InvManagementMasterEnv(periods=30, c=[4, 3, 2], I0=[3, 2, 2], dist_param={'mu': 1.2}, L=[1, 1, 1])\n",
    "\n",
    "    N_episodes = 10 # NOTE: we chose this\n",
    "    total_reward_dummy = np.array([])\n",
    "\n",
    "    for i in range(N_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        #obs, reward, done = env.sample_action()\n",
    "        \n",
    "        while not done:\n",
    "            #print(env.I)\n",
    "            action = base_stock_action(env, base_stock)\n",
    "\n",
    "            # Take the defined action (place an order), and advance to the next time period by taking a \"step\"\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            total_reward_dummy = np.append(total_reward_dummy, [reward])\n",
    "            \n",
    "    avg_total_rewards = np.average(total_reward_dummy)\n",
    "    \n",
    "    return avg_total_rewards\n",
    "\n",
    "\n",
    "\n",
    "n_restarts = 100\n",
    "best_base_stock = -99999\n",
    "\n",
    "for i in range(n_restarts):\n",
    "    x0 = np.array([random.randint(0, 10) for i in range(3)])\n",
    "    opt_base_stock = minimize(base_stock_black_box, x0)\n",
    "    #print(opt_base_stock.fun)\n",
    "    \n",
    "    if opt_base_stock.fun > best_base_stock:\n",
    "        best_base_stock = opt_base_stock.fun\n",
    "\n",
    "best_base_stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimised Min-Max Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_black_box(bounds):\n",
    "    z_min = bounds[:int(len(bounds)/2)]\n",
    "    z_max = bounds[int(len(bounds)/2):]\n",
    "    \n",
    "    env = InvManagementMasterEnv(periods=30, c=[4, 3, 2], I0=[3, 2, 2], dist_param={'mu': 1.2}, L=[1, 1, 1])\n",
    "\n",
    "    N_episodes = 10 # NOTE: we chose this\n",
    "    total_reward_dummy = np.array([])\n",
    "\n",
    "    for i in range(N_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        #obs, reward, done = env.sample_action()\n",
    "        \n",
    "        while not done:\n",
    "            #print(env.I)\n",
    "            action = min_max_action(env, z_min, z_max)\n",
    "\n",
    "            # Take the defined action (place an order), and advance to the next time period by taking a \"step\"\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            total_reward_dummy = np.append(total_reward_dummy, [reward])\n",
    "            \n",
    "    avg_total_rewards = np.average(total_reward_dummy)\n",
    "    \n",
    "    return avg_total_rewards\n",
    "\n",
    "\n",
    "n_restarts = 100\n",
    "best_min_max = -99999\n",
    "\n",
    "for i in range(n_restarts):    \n",
    "    z_up = np.array([random.randint(0, 10) for i in range(3)])\n",
    "    z_low = np.array([random.randint(0, z_up[int(i)]) for i in range(len(z_up))])\n",
    "    x0 = np.concatenate((z_low, z_up))\n",
    "    \n",
    "    opt_min_max = minimize(min_max_black_box, x0)\n",
    "    #print(opt_base_stock.fun)\n",
    "    \n",
    "    if opt_min_max.fun > best_min_max:\n",
    "        best_min_max = opt_min_max.fun\n",
    "\n",
    "best_min_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal (R, S) Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_S_black_box(inputs):\n",
    "    R_period = inputs[0]\n",
    "    S = inputs[1:]\n",
    "    \n",
    "    env = InvManagementMasterEnv(periods=30, c=[4, 3, 2], I0=[3, 2, 2], dist_param={'mu': 1.2}, L=[1, 1, 1])\n",
    "\n",
    "    N_episodes = 10 # NOTE: we chose this\n",
    "    total_reward_dummy = np.array([])\n",
    "\n",
    "    for i in range(N_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        #obs, reward, done = env.sample_action()\n",
    "        \n",
    "        while not done:\n",
    "            #print(env.I)\n",
    "            action = R_S_action(env, R_period, S)\n",
    "\n",
    "            # Take the defined action (place an order), and advance to the next time period by taking a \"step\"\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            total_reward_dummy = np.append(total_reward_dummy, [reward])\n",
    "            \n",
    "    avg_total_rewards = np.average(total_reward_dummy)\n",
    "    \n",
    "    return avg_total_rewards\n",
    "\n",
    "\n",
    "n_restarts = 100\n",
    "best_R_S = -99999\n",
    "\n",
    "for i in range(n_restarts):    \n",
    "    R_guess = np.array([random.randint(1, 5)])\n",
    "    S_guess = np.array([random.randint(0, 10) for i in range(3)])\n",
    "    x0 = np.concatenate((R_guess, S_guess))\n",
    "    \n",
    "    opt_R_S = minimize(R_S_black_box, x0)\n",
    "    #print(opt_base_stock.fun)\n",
    "    \n",
    "    if opt_R_S.fun > best_R_S:\n",
    "        best_R_S = opt_R_S.fun\n",
    "\n",
    "best_R_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal (R, s, S) Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_s_S_black_box(inputs):\n",
    "    R_period = inputs[0]\n",
    "    s = inputs[1:-3]\n",
    "    S = inputs[-3:]\n",
    "    \n",
    "    env = InvManagementMasterEnv(periods=30, c=[4, 3, 2], I0=[3, 2, 2], dist_param={'mu': 1.2}, L=[1, 1, 1])\n",
    "\n",
    "    N_episodes = 10 # NOTE: we chose this\n",
    "    total_reward_dummy = np.array([])\n",
    "\n",
    "    for i in range(N_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        #obs, reward, done = env.sample_action()\n",
    "        \n",
    "        while not done:\n",
    "            #print(env.I)\n",
    "            action = R_s_S_action(env, R_period, s, S)\n",
    "\n",
    "            # Take the defined action (place an order), and advance to the next time period by taking a \"step\"\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            total_reward_dummy = np.append(total_reward_dummy, [reward])\n",
    "            \n",
    "    avg_total_rewards = np.average(total_reward_dummy)\n",
    "    \n",
    "    return avg_total_rewards\n",
    "\n",
    "\n",
    "n_restarts = 100\n",
    "best_R_s_S = -99999\n",
    "\n",
    "for i in range(n_restarts):    \n",
    "    R_guess = np.array([random.randint(1, 5)])\n",
    "    S_guess = np.array([random.randint(0, 10) for i in range(3)])\n",
    "    s_guess = np.array([random.randint(0, z_up[int(i)]) for i in range(len(z_up))])\n",
    "    x0 = np.concatenate((R_guess, s_guess, S_guess))\n",
    "    \n",
    "    opt_R_s_S = minimize(R_s_S_black_box, x0)\n",
    "    #print(opt_base_stock.fun)\n",
    "    \n",
    "    if opt_R_s_S.fun > best_R_s_S:\n",
    "        best_R_s_S = opt_R_s_S.fun\n",
    "\n",
    "best_R_s_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal (r, Q) Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_Q_black_box(inputs):\n",
    "    r = inputs[:int(len(inputs)/2)]\n",
    "    Q = inputs[int(len(inputs)/2):]\n",
    "    \n",
    "    env = InvManagementMasterEnv(periods=30, c=[4, 3, 2], I0=[3, 2, 2], dist_param={'mu': 1.2}, L=[1, 1, 1])\n",
    "\n",
    "    N_episodes = 10 # NOTE: we chose this\n",
    "    total_reward_dummy = np.array([])\n",
    "\n",
    "    for i in range(N_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        #obs, reward, done = env.sample_action()\n",
    "        \n",
    "        while not done:\n",
    "            #print(env.I)\n",
    "            action = r_Q_action(env, r, Q)\n",
    "\n",
    "            # Take the defined action (place an order), and advance to the next time period by taking a \"step\"\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            total_reward_dummy = np.append(total_reward_dummy, [reward])\n",
    "            \n",
    "    avg_total_rewards = np.average(total_reward_dummy)\n",
    "    \n",
    "    return avg_total_rewards\n",
    "\n",
    "\n",
    "n_restarts = 100\n",
    "best_r_Q = -99999\n",
    "\n",
    "for i in range(n_restarts):    \n",
    "    r_guess = np.array([random.randint(1, 5) for i in range(3)])\n",
    "    Q_guess = np.array([random.randint(0, 10) for i in range(3)])\n",
    "    x0 = np.concatenate((r_guess, Q_guess))\n",
    "    \n",
    "    opt_r_Q = minimize(r_Q_black_box, x0)\n",
    "    #print(opt_base_stock.fun)\n",
    "    \n",
    "    if opt_r_Q.fun > best_r_Q:\n",
    "        best_r_Q = opt_r_Q.fun\n",
    "\n",
    "best_r_Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InvManagementMasterEnv(periods=30, c=[4, 3, 2], I0=[3, 2, 2], dist_param={'mu': 1.2}, L=[1, 1, 1])\n",
    "\n",
    "N_episodes = 10 # NOTE: we chose this\n",
    "total_reward_dummy = np.array([])\n",
    "\n",
    "for i in range(N_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    #obs, reward, done = env.sample_action()\n",
    "    \n",
    "    while not done:\n",
    "        #print(env.I)\n",
    "        action = env.sample_action()\n",
    "\n",
    "        # Take the defined action (place an order), and advance to the next time period by taking a \"step\"\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward_dummy = np.append(total_reward_dummy, [reward])\n",
    "        \n",
    "avg_total_rewards = np.average(total_reward_dummy)\n",
    "avg_total_rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
